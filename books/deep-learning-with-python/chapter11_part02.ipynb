{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guumaster/colab-nbs/blob/master/books/deep-learning-with-python/chapter11_part02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8bf804e-3726-4a08-b8e3-9d2575a84dc3",
      "metadata": {
        "tags": [],
        "id": "e8bf804e-3726-4a08-b8e3-9d2575a84dc3"
      },
      "source": [
        "## Sequence Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff072674-0197-49be-a0fe-155bfd0afa7c",
      "metadata": {
        "id": "ff072674-0197-49be-a0fe-155bfd0afa7c"
      },
      "source": [
        "## Preparing the IMDB movie reviews data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "dd26475f-e4b8-4653-aff8-37f8a6f8388c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd26475f-e4b8-4653-aff8-37f8a6f8388c",
        "outputId": "a75be970-cfa6-4413-e025-49fbd355ea93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  10.0M      0  0:00:07  0:00:07 --:--:-- 17.5M\n",
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "!rm -r aclImdb/train/unsup\n",
        "!cat aclImdb/train/pos/4077_10.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "35af078b-1fca-474d-808b-e31d096c3d3c",
      "metadata": {
        "id": "35af078b-1fca-474d-808b-e31d096c3d3c"
      },
      "outputs": [],
      "source": [
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category, exist_ok=True)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff723a2a-c7bc-4bfc-a9a7-97820961f8bc",
      "metadata": {
        "id": "ff723a2a-c7bc-4bfc-a9a7-97820961f8bc"
      },
      "source": [
        "### Preparing integer sequence datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "92e7d6fa-f5f7-4a65-82f4-a4f7951c8c71",
      "metadata": {
        "id": "92e7d6fa-f5f7-4a65-82f4-a4f7951c8c71"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a042b3e8-0742-41e5-88d8-9441b83e2f25",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a042b3e8-0742-41e5-88d8-9441b83e2f25",
        "outputId": "aa08b16e-eb91-433a-d3d0-f2c2fc9306ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "batch_size=32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory('aclImdb/train', batch_size=batch_size)\n",
        "val_ds = keras.utils.text_dataset_from_directory('aclImdb/val', batch_size=batch_size)\n",
        "test_ds = keras.utils.text_dataset_from_directory('aclImdb/test', batch_size=batch_size)\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b4fe4c1c-b78e-400f-ade5-1721bec40de5",
      "metadata": {
        "id": "b4fe4c1c-b78e-400f-ade5-1721bec40de5"
      },
      "outputs": [],
      "source": [
        "max_length=600\n",
        "max_tokens=20000\n",
        "\n",
        "text_vectorization = layers.TextVectorization(\n",
        "        max_tokens=max_tokens,\n",
        "        output_mode=\"int\",\n",
        "        output_sequence_length=max_length,\n",
        ")\n",
        "\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(lambda x,y: (text_vectorization(x), y), num_parallel_calls=6)\n",
        "int_val_ds = val_ds.map(lambda x,y: (text_vectorization(x), y), num_parallel_calls=6)\n",
        "int_test_ds = test_ds.map(lambda x,y: (text_vectorization(x), y), num_parallel_calls=6)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4fc2db1-6beb-48ce-ac86-3fd101c6f35d",
      "metadata": {
        "id": "d4fc2db1-6beb-48ce-ac86-3fd101c6f35d"
      },
      "source": [
        "### A sequence model built on one-hot encoded vector sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "74b8ce90-496f-49a1-b984-20445dbaf06c",
      "metadata": {
        "id": "74b8ce90-496f-49a1-b984-20445dbaf06c",
        "outputId": "c37bdf0e-48f7-4721-b974-2ae7193e718d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " tf.one_hot (TFOpLambda)     (None, None, 20000)       0         \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 64)               5128448   \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,128,513\n",
            "Trainable params: 5,128,513\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = tf.one_hot(inputs, depth=max_tokens)\n",
        "\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\", save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "id": "D7NkA_GC7LuA"
      },
      "id": "D7NkA_GC7LuA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "id": "VPeZgDiG-m9A",
        "outputId": "bfc34f81-47f9-4e8e-8335-3bbf72c74ac1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "VPeZgDiG-m9A",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 103s 131ms/step - loss: 0.3029 - accuracy: 0.8782\n",
            "Test acc: 0.878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding word embeddings\n",
        "### Learning word embeddings with the Embedding layer"
      ],
      "metadata": {
        "id": "cxeQUwQV7u7_"
      },
      "id": "cxeQUwQV7u7_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model that uses an Embedding layer trained from scratch (with masking)"
      ],
      "metadata": {
        "id": "613_ZWMY8LR_"
      },
      "id": "613_ZWMY8LR_"
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n"
      ],
      "metadata": {
        "id": "gPZXcGQp8NB5"
      },
      "id": "gPZXcGQp8NB5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(\"embeddings_bidir_gru.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\n"
      ],
      "metadata": {
        "id": "8p7vbNzEBcVB",
        "outputId": "b52253c5-238d-48bf-82ec-924dce0990d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "8p7vbNzEBcVB",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 28s 32ms/step - loss: 0.2961 - accuracy: 0.8766\n",
            "Test acc: 0.877\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using pretrained word embeddings"
      ],
      "metadata": {
        "id": "oTnCP8XS_TGq"
      },
      "id": "oTnCP8XS_TGq"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "metadata": {
        "id": "b3Bksokn_Uu5"
      },
      "id": "b3Bksokn_Uu5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "path_to_glove_file = \"glove.6B.100d.txt\"\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(f\"Found {len(embeddings_index)} word vectors.\")"
      ],
      "metadata": {
        "id": "x2pIyn97_aKN"
      },
      "id": "x2pIyn97_aKN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preparing the GloVe word-embeddings matrix"
      ],
      "metadata": {
        "id": "w-mOLwf3_cG_"
      },
      "id": "w-mOLwf3_cG_"
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "# text_vectorization is adapted to imdb train dataset\n",
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "\n",
        "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
        "\n",
        "embedding_matrix = np.zeros((max_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "  if i < max_tokens:\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector\n",
        "    "
      ],
      "metadata": {
        "id": "46S3_BsQ_eBl"
      },
      "id": "46S3_BsQ_eBl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = layers.Embedding(\n",
        "    max_tokens,\n",
        "    embedding_dim,\n",
        "    embedding_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        "    mask_zero=True,\n",
        ")"
      ],
      "metadata": {
        "id": "_pSCPQ5rBgCR"
      },
      "id": "_pSCPQ5rBgCR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model that uses a pretrained Embedding layer"
      ],
      "metadata": {
        "id": "pom9A86aB1El"
      },
      "id": "pom9A86aB1El"
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = embedding_layer(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"glove_embeddings_sequence_model.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)"
      ],
      "metadata": {
        "id": "YeWgGIsUB2hA"
      },
      "id": "YeWgGIsUB2hA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(\"glove_embeddings_sequence_model.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "id": "SfqHHJCoB78g"
      },
      "id": "SfqHHJCoB78g",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}